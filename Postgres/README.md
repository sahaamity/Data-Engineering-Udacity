##Introduction

A startup called 'Sparkify' wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to.  The goal is of this project is to enable the Analytics team by creating a Postgres database schema and ETL pipeline for the analysis. 

##Available Datasets 

######Song Datasets :  The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 
 
######Log Datasets : The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above.

##Project Description

Broadly, there are two main parts of this project :
1.Define fact and dimension tables for a star schema .
2.Write an ETL pipeline .

##Schema Design

Here ' Star Schema' has been used. There are one fact and four dimension tables as below :
• The fact table 'songplays' stores the records in log data associated with song plays i.e. records with page.
• The dimension table 'users' stores the users in the app.
• The dimension table 'song'stores the songs in the music database.
• The dimension table 'artists' stores the artists the in music database.
• The dimension table 'time' stores the timestamps of records in 'songplays' broken down into specific units.
ETL Pipeline
We have created an ETL pipeline that collects data from the files and inserts them into respective tables . "etl.py" file consists of the complete pipeline

##Project Files 

There are seven files in this project :
1.data - This folder contains the log and song datasets.
2.etl.ipynb - This is a jupyter notebook which was used to create the skeleton for the pipeline. It is kind of a workbook.
3.test.ipynb - This jupyter notebook checks whether the written scripts for creating tables and inserting data are working fine or not.
4.create_tables.py - This program contains PostgreSQL queries for creating the database and tables.
5.etl.py - This script contains the complete ETL pipeline for the project.
6.Readme.md - Documentation regarding the project.
7.sql_queries.py - This python script contains the create and insert contains for the database.

##Steps to follow :

1.Ensure that you have all of the above files in one place.
2.Open a terminal, run create_tables.py script by typing "python create_tables.py" in the terminal. This will create new databases ( and drop previous databases if any)
3.Next, run etl.py script by typing "python etl.py". This will run the ETL pipeline and extract data from the song and log datasets and insert them into the fact and dimension tables.

